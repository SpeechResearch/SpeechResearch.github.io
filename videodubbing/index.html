<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.88.1" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="../css/normalize.css">
<link rel="stylesheet" href="../css/skeleton.css">
<link rel="stylesheet" href="../css/custom.css">
<link rel="alternate" href="index.xml" type="application/rss+xml" title="Speech Research">
<link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<title>Machine Translation with Speech-Aware Length Control for Video Dubbing - Speech Research</title>
</head>
<body>

<div class="container">

	<header role="banner">
		
			
		
		
	</header>


	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
            <h1 class="entry-title" itemprop="headline">Machine Translation with Speech-Aware Length Control for Video Dubbing</h1>
			
			<section itemprop="entry-text">
				<!-- ArXiv: [arXiv:2103.00993](https://arxiv.org/pdf/2103.00993.pdf) -->
<h2 id="authors">Authors</h2>
<ul>
<li>Yihan Wu (Gaoling School of Artificial Intelligence, Renmin University of China) <a href="mailto:yihanwu@ruc.edu.cn">yihanwu@ruc.edu.cn</a></li>
<li>Junliang Guo (Microsoft Research Asia) <a href="mailto:junliangguo@microsoft.com">junliangguo@microsoft.com</a></li>
<li>Xu Tan (Microsoft Research Asia) <a href="mailto:xuta@microsoft.com">xuta@microsoft.com</a></li>
<li>Chen Zhang (Microsoft Azure Speech) <a href="mailto:zhangche@microsoft.com">zhangche@microsoft.com</a></li>
<li>Bohan Li (Microsoft Azure Speech) <a href="mailto:bohli@microsoft.com">bohli@microsoft.com</a></li>
<li>Ruihua Song (Gaoling School of Artificial Intelligence, Renmin University of China) <a href="mailto:rsong@ruc.edu.cn">rsong@ruc.edu.cn</a></li>
<li>Lei He (Microsoft Azure Speech) <a href="mailto:helei@microsoft.com">helei@microsoft.com</a></li>
<li>Sheng Zhao (Microsoft Azure Speech) <a href="mailto:szhao@microsoft.com">szhao@microsoft.com</a></li>
<li>Arul Menezes (Microsoft Azure Translation) <a href="mailto:arulm@microsoft.com">arulm@microsoft.com</a></li>
<li>Jiang Bian (Microsoft Research) <a href="mailto:jiang.bian@microsoft.com">jiang.bian@microsoft.com</a></li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>Video dubbing aims to translate the original speech in a film or television program into the speech in a target language,
which can be achieved with a cascaded system consisting of speech recognition, machine translation and speech synthesis. To ensure the translated speech to be well aligned with the corresponding video, the length/duration of the translated speech should be as close as possible to that of the original speech, which requires strict length control. Previous works usually control the number of words or characters generated by the machine translation model to be similar to the source sentence, without considering the isochronicity of speech as the speech duration of words/characters in different languages varies. In this paper, we propose a machine translation system tailored for the task of video dubbing, which directly considers the speech duration of each token in translation, to match the length of source and target speech. Specifically, we control the speech length of generated sentence by guiding the prediction of each word with the duration information, including the speech duration of itself as well as how much duration is left for the remaining words. We design experiments on four language directions (German→English, Spanish→English, Chinese↔English), and the results show that the proposed method achieves better length control ability on the generated speech than baseline methods. To make up the lack of real-world datasets, we also construct a real-world test set collected from films to provide comprehensive evaluations on the video dubbing task.</p>
<h2 id="model-architecture">Model Architecture</h2>
<img src="../images/videodubbing/arch.png" width="100%"/>
<h2 id="task-overview">Task Overview</h2>
<img src="../images/videodubbing/task.png" width="100%"/>
<p><span id="audio-samples" name="audio-samples"></span></p>
<h2 id="audio-samples">Demo Video (translate from Chinese to English)</h2>
The below video is used to show the comparison of our method with other baseline methods.
<p></p>
<td style="text-align: center"><video width="100%" controls>
  <source src="../images/videodubbing/dubbed_video_zhen.mp4" type="video/mp4">
您的浏览器不支持 video 标签。
</video></td>
<p><span id="audio-samples" name="audio-samples"></span></p>
<h2 id="audio-samples">Demo Video (translate from English to Chinese)</h2>
The below video is used to show the comparison of our method with other baseline methods.
<p></p>
<td style="text-align: center"><video width="100%" controls>
  <source src="../images/videodubbing/dubbed_video_enzh.mp4" type="video/mp4">
您的浏览器不支持 video 标签。
</video></td>
<p><span id="audio-samples" name="audio-samples"></span></p>
<h2 id="audio-samples">Demo Video (compared with other methods)</h2>
The below video is used to show the comparison of our method with other baseline methods.
<p></p>
<td style="text-align: center"><video width="100%" controls>
  <source src="../images/videodubbing/dubbed_video.mp4" type="video/mp4">
您的浏览器不支持 video 标签。
</video></td>
<p><span id="experiment" name="experiment"></span></p>
<h2 id="audio-samples">Real-World Video Dubbing Test Set</h2>
<p>Considering the scarcity of real-world video dubbing dataset (i.e., motion pictures with golden cross-lingual source and target speech), we construct a test set collected from dubbed films to provide comprehensive evaluations of video dubbing systems. </p>
<p>Specifically, we select nine popular films translated from English to Chinese, which are of high manual translation and dubbing quality, and contain rich genres such as love, action, scientific fiction, etc. We cut 42 conversation clips from them with the following criteria: 1) The clip duration is around 1 ∼ 3 minutes. 2) More than 10 sentences are involved in each clip, which contains both long and short sentences. 3) The face of speaker is visible mostly during his or her talks, especially visible lips at the end of speech.</p>
Please follow <a href="../images/videodubbing/testset.txt">here</a> for more information about <em>Real-World Video Dubbing Test Set</em>.
<p><span id="experiment" name="experiment"></span></p>
<h2 id="audio-samples">Experiment Results</h2>
<h3 id="audios-of-ablation-study-on-vctk">1. Automatic Evaluation</h3>
We show the machine translation quality and the length control performance of related models on four language directions in Table 1. Besides, we also list the results when controlling the translation with the length of the golden target speech, to show the upper-bound performance of our model.
<img src="../images/videodubbing/tab1.png" width="100%"/>
<h3 id="audios-of-ablation-study-on-vctk">2. Ablation Study</h3>
<p>To verify the effectiveness of the proposed duration-aware positional embedding, we conduct ablation studies on three kinds of PEs on four language directions, as shown in Table 2. We can find that the absolute and relative duration PE are both crucial to achieve better speech-aware length control results.</p>
<img src="../images/videodubbing/tab2.png" width="100%"/>
<h3 id="audios-of-ablation-study-on-vctk">3. Results on Real-world Test Set</h3>
<p>To compare the performance of related methods on the realworld video dubbing scenario, we conduct experiments on the real-world test set constructed by us. Results are shown in Table 3.</p>
<p>We conduct subjective evaluation to evaluate the translation quality, the synchronization with the original film footage, and the overall quality of the synthesized speech. We hired 8 judges who are good at both Chinese and English, asking them to rate the samples generated from different method on the 5 scale.</p>
<img src="../images/videodubbing/tab3.png" width="100%"/>
<h2 id="our-related-works">Our Related Works</h2>
<p><a href="/adaspeech/">AdaSpeech: Adaptive Text to Speech for Custom Voice</a><br>
<a href="/adaspeech2/">AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data</a><br>
<a href="/adaspeech3/">AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style</a><br>
<a href="/adaspeech3/">AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios</a><br>
<a href="/fastspeech/">FastSpeech: Fast, Robust and Controllable Text to Speech</a><br>
<a href="/fastspeech2/">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a><br>
<a href="/multispeech/">MultiSpeech: Multi-Speaker Text to Speech with Transformer</a><br>
<a href="/lrspeech/">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a><br></p>

			</section>
		</article>
	</main>


	

</div>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139981676-1', 'auto');
	ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>




</body>
</html>

